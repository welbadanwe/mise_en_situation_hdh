{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installer pyspark dans mon environnement virtuel\n",
    "    #!pip install pyspark\n",
    "    #!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ETL OMOP\").getOrCreate()\n",
    "\n",
    "#recuperer les données\n",
    "specialite_info = spark.read.csv(\"ir_spe_v.csv\", header=True, inferSchema=True)\n",
    "actes_infos = spark.read.csv(\"ir_act_v.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#creer une table unifiée des spécialités, par l'union \n",
    "provider_tab = specialite_info.select(\n",
    "    col(\"pfs_spe_cod\").alias(\"spe_cod\"),\n",
    "    col(\"label\")\n",
    ").union(\n",
    "    actes_infos.select(\n",
    "        col(\"pfs_act_nat\").alias(\"spe_cod\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# gerer provider_id par des chiffres auto-incrémentés et je les ajoute à la table\n",
    "#les autres tables aussi\n",
    "window = Window.orderBy(\"spe_cod\")\n",
    "provider_tab = provider_tab.withColumn(\"provider_id\", row_number().over(window))\n",
    "provider_tab = provider_tab.withColumn(\"provider_source_value\", col(\"spe_cod\"))\n",
    "\n",
    "# specialty_source_value sera ma liste de specialistes\n",
    "## Hypothese : le type de la colonne est varchar, et selon la documentation OMOP \n",
    "###il peut s'agir soit d'un code du spécialiste, soit le texte de description\n",
    "provider_tab = provider_tab.withColumn(\"specialty_source_value\", col(\"label\"))\n",
    "\n",
    "#specialty_concept_id, je pars du principe qu'il manque des données de \"sourceCode\", j'impute la valeur 0\n",
    "provider_tab = provider_tab.withColumn(\"specialty_concept_id\", lit(0).cast(\"integer\"))\n",
    "\n",
    "#créer la table provider\n",
    "provider = provider_tab.select(\"provider_id\", \"specialty_source_value\", \"specialty_concept_id\", \"provider_source_value\")\n",
    "\n",
    "# Sauvegarder au format Parquet\n",
    "provider.write.mode(\"overwrite\").parquet(\"provider_table.parquet\")\n",
    "\n",
    "# Afficher quelques lignes pour vérifier\n",
    "provider.show(10, truncate=False)\n",
    "# Fermer Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
